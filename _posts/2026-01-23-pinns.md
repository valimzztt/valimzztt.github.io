---
layout: post
title: "Physics-Informed Neural Networks (PINNs)"
date: 2026-01-23
categories: [projects]
author: Valentina Mazzotti
tags: [neural-networks, machine-learning]
---

Recently, during my internship at the Fraunhofer Institute for Integrated Systems and Device Technology (IISB), I explored Physics-Informed Neural Networks (PINNs), which are a class of neural networks that embed physical laws directly into the learning process and can be described by partial differential equations (PDEs).

Unlike standard machine learning models, PINNs do not rely primarily on labeled data. Instead, they incorporate governing equations, typically partial differential equations (PDEs), directly into the loss function. In doing so, they constrain the solution space to functions that obey known physics. This is interesting because it changes the question from *"Can we fit this data?" * to *"Can we fit this data without violating the laws of physics?"*. The idea behind PINNs is that we can use existing physics to help guide our model. Basically when we have data, and some known physics, how do we incorporate, or more importantly, make sure we don’t violate the known physics when fitting said data.

Before we start going into what my project was about, let's start to introduce the concept of neural network in general. 
A neural network is a universal function approximator:

$$
u_\theta(\mathbf{x}) \approx u(\mathbf{x})
$$

where $\theta$ are trainable parameters.

In a PINN, we assume the function satisfies a PDE of the form:

$$
\mathcal{N}[u(\mathbf{x})] = 0
$$

where $\mathcal{N}$ is a differential operator.

Instead of training purely on data, we define a **physics residual**:

$$
\mathcal{R}_\theta(\mathbf{x}) = \mathcal{N}[u_\theta(\mathbf{x})]
$$

The loss function becomes:

$$
\mathcal{L} =
\lambda_f \| \mathcal{R}_\theta(\mathbf{x}) \|^2
+
\lambda_b \| \text{BC residual} \|^2
+
\lambda_d \| \text{data mismatch} \|^2
$$

where:

- The first term enforces the PDE  
- The second term enforces boundary/initial conditions  
- The third term optionally incorporates data (in case you have some)

So basically, the core of the PINN approach is that the prior knowledge of general physical laws acts in the training of neural networks (NNs) as a regularization agent that limits the space of admissible solutions, increasing the generalizability of the function approximation.

<figure>
  <img src="/assets/images/figs_pinns/IntroductionToPinns.jpg" alt="PINNs" style="width:80%">
</figure>


For many problems, we know part of the physics but not all of it.

You can think of it as:

$$
\text{Reality} = \text{Known physics} + \text{unknown corrections}
$$

Can we:

- Encode what we *do* know?
- Learn only the residual?
- Extract low-rank structure?
- Remain robust when data is noisy or expensive?

This makes PINNs especially attractive in scientific computing and quantum simulation. In fact, most early PINN applications focused on fluid dynamics (for example the Navier–Stokes equations). But what about quantum mechanics?

If there is one equation at the heart of quantum physics, it is the **time-independent Schrödinger equation**:

$$
\hat{H}\psi = E\psi
$$

This is an eigenvalue problem: the Hamiltonian operator $\hat{H}$ acts on a wavefunction $\psi$, producing energy eigenvalues $E$.

The million question now is whether a neural network could *learn* both $\psi$ and $E$?

---

# Solving the Ground State of the Hydrogen Atom

The hydrogen atom is the ideal test system to test PINNs on a quantum problem: it is analytically solvable, meaning that an analytical solution exists, yet it is nontrivial.

The time-independent Schrödinger equation reads:

$$
\left[
-\frac{\hbar^2}{2m} \nabla^2
-\frac{e^2}{4\pi\varepsilon_0 r}
\right]
\psi(\mathbf{r})
=
E \psi(\mathbf{r})
$$

Using atomic units ($\hbar = m = e = 1$), this simplifies to:

$$
\left[
-\frac{1}{2} \nabla^2
-\frac{1}{r}
\right]
\psi(\mathbf{r})
=
E \psi(\mathbf{r})
$$


Hydrogen wavefunctions separate into radial and angular parts:

$$
\psi_{n\ell m}(r,\theta,\phi)
=
R_{n\ell}(r) Y_{\ell m}(\theta,\phi)
$$

For the ground state ($n=1, \ell=0$):

$$
\psi_{100}(r) =
\frac{1}{\sqrt{\pi}} e^{-r}
$$

with energy:

$$
E_0 = -\frac{1}{2} \quad \text{Hartree}
$$

This exponential decay, which is a smooth yet sharply curved near the origin, is a nontrivial function to approximate, for which one needs to take some additional care. 

Regarding the PINN, architecture, I used a fully connected feedforward neural network with three hidden layers. Initially, I used leaky ReLU (or “sneaky ReLU”) as the activation function. However, I iscovered that it was not well-suited for this task, as switching to Tanh activations improved the performance significantly. The Tanh nonlinearity better captured the smooth, sinusoidal nature of the solution and allowed the network to approximate second derivatives more accurately. This highlighted a key lesson in PINNs: activation functions matter, especially when the target function requires higher-order differentiability. I benchmarked different network architectures, comparing a "Small" network (16 neurons, 1 hidden layer) against a "Deep" network (64 neurons, 3 hidden layers). The deeper network was essential for capturing the sharp exponential decay of the 1s orbital.

A significant challenge was the singularity of the Coulomb potential ($V(r) \sim -1/r$) at the origin. To handle this, I had to be careful with sampling points near $r=0$, and I overcame the problem by sampling on a logarithmic scae.
The neural network outputs:

- $\psi_\theta(\mathbf{r})$
- $E_\theta$ (a trainable scalar)

The residual becomes:

$$
\mathcal{R}_\theta(\mathbf{r}) =
\left(
-\frac{1}{2}\nabla^2 \psi_\theta
-\frac{1}{r}\psi_\theta
\right)
- E_\theta \psi_\theta
$$

The physics-informed loss:

$$
\mathcal{L} =
\left\|
\mathcal{R}_\theta(\mathbf{r})
\right\|^2
$$

Minimizing this forces the network to satisfy:

$$
\hat{H}\psi = E\psi
$$
The loss function was then minimized to satisfy the energy eigenvalue equation:
$$\hat{H}\psi = E\psi$$
where the network outputs both the wavefunction $\psi$ and the energy eigenvalue $E$ as a trainable parameter.

<figure>
  <img src="/assets/images/figs_pinns/1DHydrogenAtom.png" alt="3D Hydrogen Atom PINN Results" style="width:100%">
  <figcaption>
    <strong>Figure 1: 3D Hydrogen Atom Results.</strong> The figure illustrates the training progress and final accuracy of the PINN solver for the 3D Hydrogen atom. <br>
    <strong>Left Panel (Energy Convergence):</strong> Shows the evolution of the predicted ground state energy ($E$) over 2000 training epochs. The learned energy (blue solid line) starts from a random initialization and rapidly converges to the exact analytical value of -0.5 Hartree (red dashed line). <br>
    <strong>Right Panel (Radial Wavefunction):</strong> Compares the PINN-predicted radial wavefunction $R(r)$ (blue solid line) with the exact analytical solution for the $1s$ orbital (red dashed line). The perfect overlap demonstrates that the network successfully learned the exponential decay characteristic of the ground state wavefunction in the radial coordinate $r$ (Bohr).
  </figcaption>
</figure>

Because the Schrödinger equation is an eigenvalue problem,

$$
\hat{H}\psi = E\psi,
$$

the energy $E$ was treated as a **trainable parameter** alongside the neural network weights.

A natural concern is whether the network simply converges to a local minimum that depends on the initial energy guess.  
To investigate this, I benchmarked the model across a range of different initialization energies. In each case, the network was initialized with a different starting value for $E_\theta$ and trained independently. What I noticed was that the model consistently converged toward the correct ground-state energy:

$$
E \rightarrow -0.5
$$

independent of the initial guess. 

## Hydrogen Molecular Ion ($H_2^+$)

The Hydrogen Molecular Ion ($H_2^+$) represents the simplest molecule and introduces the challenge of multi-center potentials, since now the electron now interacts with two protons, creating a more complex potential landscape.

To model the ground state of the $H_2^+$ ion, we use the **Linear Combination of Atomic Orbitals (LCAO)** method. The trial wavefunction is constructed as a superposition of two $1s$ hydrogenic orbitals centered at the nuclei positions, $\mathbf{R}_A$ and $\mathbf{R}_B$:

$$
\psi_{LCAO}(\mathbf{r}) = N \left[ \phi_{1s}(\mathbf{r} - \mathbf{R}_A) + \phi_{1s}(\mathbf{r} - \mathbf{R}_B) \right]
$$

where the spatial orbital $\phi_{1s}$ includes the trainable decay parameter $\alpha$:

$$
\phi_{1s}(\mathbf{r}) = \sqrt{\frac{\alpha^3}{\pi}} e^{-\alpha |\mathbf{r}|}
$$

The normalization constant $N$ depends on the internuclear distance $R$ and the overlap integral $S$:

$$
N = \frac{1}{\sqrt{2\pi (1 + S)}}
$$

For the specific case of **equilibrium bond length ($R = 2\ a_0$)**, the overlap integral $S$ (assuming $\alpha \approx 1$) is calculated as:

$$
S = e^{-R} \left( 1 + R + \frac{R^2}{3} \right)
$$

Plugging in $R=2$, we find a significant overlap ($S \approx 0.586$), indicating strong quantum mechanical sharing of the electron between the two protons—the essence of the chemical bond.

The plot above compares the probability density ($\psi^2$) predicted by the Physics-Informed Neural Network (PINN) against the analytical LCAO reference along the internuclear axis ($z$). 

For this system, I investigated two distinct training strategies to improve the robustness of the model: 
1.  **Trainable Orbital Exponents:** Instead of a black-box network, I incorporated physical intuition by using an ansatz with a trainable orbital exponent $\alpha$ (similar to a Slater-type orbital). This allowed the network to learn the "decay rate" of the wavefunction explicitly.
2.  **Rayleigh Quotient vs. Trainable Energy:** I compared treating the energy $E$ as a simple network parameter versus computing it via the **Rayleigh Quotient** at each step. The Rayleigh Quotient method, while computationally more expensive, often provided a more stable and variationally bound estimate of the ground state energy.

<figure>
  <img src="/assets/images/figs_pinns/2Hplus.png" alt="PINNs3" style="width:80%">
</figure>

The figure above shows the learned wavefunction $\psi(z)$ of the hydrogen molecular ion $H_2^+$ along the internuclear axis, compared against a reference LCAO solution, and we can immediately observe the following things: 

- **Cusps at the nuclei**: The sharp peaks at $$z = \pm \frac{R}{2}$$ correspond to the positions of the two protons. The wavefunction must exhibit a “cusp”, namely a discontinuity in the derivative at these points due to the singularity in the Coulomb potential $$ V(r) = -\frac{1}{r}$$. The PINN (solid blue) learns this derivative discontinuity without explicitly enforcing it, closely matching the LCAO reference (orange dashed).

- **Electron delocalization:**: he region between the peaks ($$ -\frac{R}{2} < z < \frac{R}{2}$$) represents the bonding region. A non-zero probability density here indicates that the electron is delocalized over both nuclei, stabilizing the molecule via covalent bonding.

The close agreement between the two curves shows that the neural network converges to the correct ground state, and is able to capture both the Coulomb-induced cusps, the exponential decay away from the nuclei as well as the tunneling behavior in the internuclear region  

  All these features emerge from just minimizing the Schrödinger residual, without any training data needed!

## What About Multi-Electron Systems?

The natural next step is the **Helium atom**, which is the simplest interacting two-electron system.

Here, we immediately encounter the infamous *curse of dimensionality*.

In a straightforward PINN formulation for Helium, the input space becomes six-dimensional:

$$
(x_1, y_1, z_1, x_2, y_2, z_2)
$$

Even for this smallest multi-electron system, the configuration space grows rapidly.  
Resolving the electron–electron interaction term

$$
\frac{1}{|\mathbf{r}_1 - \mathbf{r}_2|}
$$

requires sufficiently dense sampling across this high-dimensional domain.

A naive collocation-based PINN approach would require an enormous number of points to properly resolve correlation effects. The computational cost scales poorly.

This raises a fundamental question:

> Are standard PINNs suitable for high-dimensional quantum systems?

At this stage, the answer is not entirely clear.

---

One possible direction I am currently investigating is whether separable neural architectures can alleviate this scaling issue.

For example:

- Using *Siamese networks* to learn single-particle orbitals,
- Introducing an explicit correlation subnetwork (Jastrow factor),
- Encoding permutation symmetry directly into the architecture.

Such approaches may reduce effective dimensionality by exploiting physical structure rather than treating the wavefunction as a generic function in $\mathbb{R}^6$.

For high-dimensional quantum systems, Neural Variational Monte Carlo (Neural VMC) has already established itself as a powerful and scalable technique. The key idea behind VMC is that quantum wavefunctions are not arbitrary high-dimensional functions, they possess symmetry, smoothness, and physical constraints that may be exploitable by design. In a Neural VMC, the electron configurations are sampled via Markov Chain Monte Carlo (MCMC) and the loss function becomes the expectation value of the local energy:

$$
\langle E_L \rangle =
\frac{\int \psi_\theta^*(\mathbf{R}) \hat{H} \psi_\theta(\mathbf{R}) \, d\mathbf{R}}
{\int |\psi_\theta(\mathbf{R})|^2 \, d\mathbf{R}}
$$

This has the advantage that physical constraints  (such as the Fermionic antisymmetry, the electron-nucleus and the electron-electron cusp conditions) can be built in explicitly. Tather than solving the differential equation directly (as in standard PINNs), VMC reframes the problem as earning a probability distribution that minimizes the variational energy, and this probabilistic formulation logically scales far better in high dimensions.

The open research question, therefore, is: *Can physics-informed neural networks be reformulated in a way that retains their PDE-driven interpretability while incorporating the sampling efficiency of VMC?*

Combining these two very different paradigms, and the strenght that each of them brings, might open up interesting avenues to explore!

