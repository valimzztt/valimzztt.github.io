---
layout: post
title: "Physics-Informed Neural Networks (PINNs)"
date: 2026-01-23
categories: [projects]
author: Valentina Mazzotti
tags: [neural-networks, machine-learning]
---

Recently, as part of my internship at the Fraunhofer Institute for Integrated Systems and Device Technology, I explored the concept of Physics-Informed Neural Networks (PINNs), a framework that merges neural networks with differential equations. Unlike traditional machine learning models, PINNs do not require labeled training data. Instead, they rely entirely on the governing physical laws, encoded as a loss function derived from the underlying differential equation and its boundary and initial conditions.

The goal of a PINN is not to classify or regress from examples but to solve a partial differential equation (PDE) over a specified domain. During training, the network adjusts its weights to minimize the residual of the PDE as well as any violations of initial and boundary constraints.

---

## Solving the 1D Hydrogen Atom

My journey began with the time-dependent Schrödinger equation (TDSE) in one dimension. This serves as a "toy model" for atoms in strong laser fields, often using a **Soft-Coulomb potential** to avoid the singularity at the origin.

The problem was set up with an initial condition $$u(x, 0) = \frac{1}{2} \sin(2\pi x)$$, and zero initial time derivative. The boundary conditions were Dirichlet ($u(0, t) = u(1, t) = 0$), effectively placing the system in an infinite potential well or "box".

I used a fully connected feedforward neural network with three hidden layers. Initially, I used leaky ReLU (or “sneaky ReLU”) as the activation function. However, I quickly discovered that it was not well-suited for this task: the resulting solution lacked the smoothness and periodic structure expected of a wave equation, and the network struggled to converge.

Switching to Tanh activations improved the performance significantly. The Tanh nonlinearity better captured the smooth, sinusoidal nature of the solution and allowed the network to approximate second derivatives more accurately. This highlighted a key lesson in PINNs: activation functions matter, especially when the target function requires higher-order differentiability.

<figure>
  <img src="assets/images/figs_pinns/1DHydrogenAtom.png" alt="3D Hydrogen Atom PINN Results" style="width:100%">
  <figcaption>
    <strong>Figure 1: 3D Hydrogen Atom Results.</strong> The figure illustrates the training progress and final accuracy of the PINN solver for the 3D Hydrogen atom. <br>
    <strong>Left Panel (Energy Convergence):</strong> Shows the evolution of the predicted ground state energy ($E$) over 2000 training epochs. The learned energy (blue solid line) starts from a random initialization and rapidly converges to the exact analytical value of -0.5 Hartree (red dashed line). <br>
    <strong>Right Panel (Radial Wavefunction):</strong> Compares the PINN-predicted radial wavefunction $R(r)$ (blue solid line) with the exact analytical solution for the $1s$ orbital (red dashed line). The perfect overlap demonstrates that the network successfully learned the exponential decay characteristic of the ground state wavefunction in the radial coordinate $r$ (Bohr).
  </figcaption>
</figure>

## The 3D Hydrogen Atom

Moving to the **3D Hydrogen Atom**, the complexity increased significantly. The goal was to solve the stationary Schrödinger equation for the ground state wavefunction $\psi(r)$.

I benchmarked different network architectures, comparing a "Small" network (16 neurons, 1 hidden layer) against a "Deep" network (64 neurons, 3 hidden layers). The deeper network was essential for capturing the sharp exponential decay of the 1s orbital.

A significant challenge was the **singularity of the Coulomb potential** ($V(r) \sim -1/r$) at the origin. To handle this, I had to be careful with sampling points near $r=0$, often using a logarithmic grid or a coordinate transformation. The loss function was minimized to satisfy the energy eigenvalue equation:
$$\hat{H}\psi = E\psi$$
where the network outputs both the wavefunction $\psi$ and the energy eigenvalue $E$ as a trainable parameter.

## The 3D H2+ Hydrogen Atom

The **Hydrogen Molecular Ion ($H_2^+$)** represents the simplest molecule and introduces a new challenge: **multi-center potentials**. The electron now interacts with two protons, creating a more complex potential landscape.

For this system, I investigated two distinct training strategies to improve robustness:

1.  **Trainable Orbital Exponents:** Instead of a black-box network, I incorporated physical intuition by using an ansatz with a trainable orbital exponent $\alpha$ (similar to a Slater-type orbital). This allowed the network to learn the "decay rate" of the wavefunction explicitly.
2.  **Rayleigh Quotient vs. Trainable Energy:** I compared treating the energy $E$ as a simple network parameter versus computing it via the **Rayleigh Quotient** at each step. The Rayleigh Quotient method, while computationally more expensive, often provided a more stable and variationally bound estimate of the ground state energy.

## What about multi-electron systems?

The natural next step was the **Helium atom**, a two-electron system. Here, I hit the "curse of dimensionality."

In a standard PINN approach for Helium, the input space becomes 6-dimensional ($x_1, y_1, z_1, x_2, y_2, z_2$). The number of collocation points required to cover this space densely enough to resolve the electron-electron interaction (correlation) explodes.

My investigations concluded that standard PINNs are inefficient for these high-dimensional quantum problems. Instead, I shifted focus towards **Neural Variational Monte Carlo (Neural VMC)**. In this approach:
* We sample electron configurations using **Markov Chain Monte Carlo (MCMC)** rather than a fixed grid.
* The loss function becomes the expectation value of the local energy, $\langle E_L \rangle$.
* We can explicitly build in physical constraints, such as the **cusp conditions** (where particles meet) and the antisymmetry required for fermions (using determinants).

This transition marks a move from "solving differential equations" (standard PINNs) to "learning the probability distribution" (VMC), which is far more scalable for multi-electron chemistry.